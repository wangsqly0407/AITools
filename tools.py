#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å‡½æ•°è°ƒç”¨æŒ‡æ ‡è¯„ä¼°å·¥å…·æ¨¡å—

è¯¥æ¨¡å—æä¾›äº†è¯„ä¼°å‡½æ•°è°ƒç”¨é¢„æµ‹ç»“æœä¸å‚è€ƒç»“æœåŒ¹é…åº¦çš„åŠŸèƒ½ã€‚
"""

import json
from typing import List, Dict, Any


def evaluate_function_calls_metrics(
        predictions: List[Dict[str, Any]], 
        references: List[Dict[str, Any]], 
        number: int = 1
) -> Dict[str, Any]:
    """
    è¯„ä¼°å‡½æ•°è°ƒç”¨æŒ‡æ ‡ï¼Œè®¡ç®—é¢„æµ‹ç»“æœä¸å‚è€ƒç»“æœçš„äº¤é›†åŠç›¸å…³æŒ‡æ ‡
    
    è¯¥å‡½æ•°æ¥æ”¶é¢„æµ‹ç»“æœå’Œå‚è€ƒç»“æœä¸¤ä¸ªåˆ—è¡¨ï¼Œæ ¹æ®å‡½æ•°åç§°ï¼ˆfunction.nameï¼‰ä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦ï¼Œ
    æ‰¾å‡ºä¸¤ä¸ªåˆ—è¡¨ä¸­çš„äº¤é›†å…ƒç´ ï¼Œè¿”å›åŒ¹é…çš„å‡½æ•°è°ƒç”¨è®°å½•ä»¥åŠå·¥å…·è¯†åˆ«ç‡å’Œè°ƒç”¨å‡†ç¡®ç‡ã€‚
    
    Args:
        predictions (List[Dict[str, Any]]): é¢„æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«å‡½æ•°è°ƒç”¨ä¿¡æ¯
            æ ¼å¼: [{
                "id": str,
                "type": "function", 
                "function": {
                    "name": str,
                    "arguments": str (JSONæ ¼å¼å­—ç¬¦ä¸²)
                }
            }, ...]
            
        references (List[Dict[str, Any]]): å‚è€ƒç»“æœåˆ—è¡¨ï¼Œæ ¼å¼ä¸predictionsç›¸åŒ
        
        number (int, optional): åˆ¤æ–­å·¥ä½œæµé€‰æ‹©æ­£ç¡®æ€§çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸º1
            å¿…é¡»ä¸ºéè´Ÿæ•°ï¼ˆ>= 0ï¼‰ï¼Œäº¤é›†ä¸ªæ•° >= number æ—¶è®¤ä¸ºé€‰æ‹©äº†æ­£ç¡®çš„å·¥ä½œæµ
    
    Returns:
        Dict[str, Any]: è¿”å›åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸:
            - "fcm": List[Dict[str, Any]] - åŒ¹é…çš„å‡½æ•°è°ƒç”¨è®°å½•åˆ—è¡¨
            - "tool_recognition_rate": bool - å·¥å…·è¯†åˆ«ç‡ï¼Œpredictionséç©ºä¸ºTrue
            - "correct_workflow_selection": bool - æ˜¯å¦é€‰æ‹©äº†æ­£ç¡®çš„å·¥ä½œæµ
            - "hallucination_rate": float - å¹»è§‰ç‡ï¼Œ
              (predictionsç‹¬æœ‰æ•°é‡)/referencesæ•°é‡
    
    Raises:
        ValueError: å½“è¾“å…¥å‚æ•°æ ¼å¼ä¸æ­£ç¡®æ—¶æŠ›å‡º
        TypeError: å½“è¾“å…¥å‚æ•°ç±»å‹ä¸æ­£ç¡®æ—¶æŠ›å‡º
    
    Examples:
        >>> predictions = [{
        ...     "id": "test-1",
        ...     "type": "function",
        ...     "function": {"name": "æ™ºèƒ½ä½“-A", "arguments": "{}"}
        ... }]
        >>> references = [{
        ...     "id": "ref-1", 
        ...     "type": "function",
        ...     "function": {"name": "æ™ºèƒ½ä½“-A", "arguments": "{}"}
        ... }]
        >>> result = evaluate_function_calls_metrics(predictions, references)
        >>> len(result["fcm"]) == 1
        True
        >>> result["tool_recognition_rate"]
        True
    """
    
    # 1. åŸºæœ¬ç±»å‹æ ¡éªŒ
    if not isinstance(predictions, list):
        raise TypeError(
            f"predictions å¿…é¡»æ˜¯ list ç±»å‹ï¼Œå½“å‰ç±»å‹: "
            f"{type(predictions).__name__}"
        )
    
    if not isinstance(references, list):
        raise TypeError(
            f"references å¿…é¡»æ˜¯ list ç±»å‹ï¼Œå½“å‰ç±»å‹: "
            f"{type(references).__name__}"
        )
    
    # 2. æ–°å¢å‚æ•°ç±»å‹æ ¡éªŒ
    if not isinstance(number, int):
        raise TypeError(
            f"number å¿…é¡»æ˜¯ int ç±»å‹ï¼Œå½“å‰ç±»å‹: "
            f"{type(number).__name__}"
        )
    
    # 3. æ ¡éªŒnumberå‚æ•°å¿…é¡»ä¸ºéè´Ÿæ•°
    if number < 0:
        raise ValueError(f"number å¿…é¡»æ˜¯éè´Ÿæ•°ï¼Œå½“å‰å€¼: {number}")
    
    # 4. è®¡ç®—å·¥å…·è¯†åˆ«ç‡
    tool_recognition_rate = len(predictions) > 0
    
    # 5. å¤„ç†ç©ºåˆ—è¡¨æƒ…å†µ
    if not predictions or not references:
        # è®¡ç®—å¹»è§‰ç‡ï¼š(predictionsç‹¬æœ‰æ•°é‡) / referencesæ•°é‡
        if len(predictions) == 0:
            # predictionsä¸ºç©ºï¼Œæ²¡æœ‰é¢„æµ‹å°±æ²¡æœ‰å¹»è§‰
            hallucination_rate = 0.0
        else:
            # predictionsä¸ä¸ºç©ºä½†referencesä¸ºç©ºï¼Œæ‰€æœ‰predictionséƒ½æ˜¯å¹»è§‰
            hallucination_rate = float('inf')
        
        return {
            "fcm": [],
            "tool_recognition_rate": tool_recognition_rate,
            # ç©ºåˆ—è¡¨æ—¶æ— æ³•é€‰æ‹©æ­£ç¡®çš„å·¥ä½œæµ
            "correct_workflow_selection": False,
            "hallucination_rate": hallucination_rate
        }
    
    # 6. éªŒè¯åˆ—è¡¨å…ƒç´ æ ¼å¼
    def validate_function_call_format(item: Any, item_type: str, index: int) -> None:
        """éªŒè¯å•ä¸ªå‡½æ•°è°ƒç”¨è®°å½•çš„æ ¼å¼"""
        if not isinstance(item, dict):
            raise ValueError(
                f"{item_type}[{index}] å¿…é¡»æ˜¯ dict ç±»å‹ï¼Œ"
                f"å½“å‰ç±»å‹: {type(item).__name__}"
            )
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ["id", "type", "function"]
        for field in required_fields:
            if field not in item:
                raise ValueError(
                    f"{item_type}[{index}] ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"
                )
        
        # æ£€æŸ¥ type å­—æ®µå€¼
        if item["type"] != "function":
            raise ValueError(
                f"{item_type}[{index}] çš„ type å­—æ®µå¿…é¡»ä¸º 'function'ï¼Œ"
                f"å½“å‰å€¼: {item['type']}"
            )
        
        # æ£€æŸ¥ function å­—æ®µæ ¼å¼
        function_obj = item["function"]
        if not isinstance(function_obj, dict):
            raise ValueError(
                f"{item_type}[{index}] çš„ function å­—æ®µå¿…é¡»æ˜¯ dict ç±»å‹"
            )
        
        function_required_fields = ["name", "arguments"]
        for field in function_required_fields:
            if field not in function_obj:
                raise ValueError(
                    f"{item_type}[{index}] çš„ function ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"
                )
        
        # éªŒè¯ arguments æ˜¯å¦ä¸ºæœ‰æ•ˆ JSON å­—ç¬¦ä¸²
        try:
            json.loads(function_obj["arguments"])
        except (json.JSONDecodeError, TypeError) as e:
            raise ValueError(
                f"{item_type}[{index}] çš„ function.arguments "
                f"ä¸æ˜¯æœ‰æ•ˆçš„ JSON å­—ç¬¦ä¸²: {e}"
            )
    
    # 7. éªŒè¯æ‰€æœ‰å…ƒç´ æ ¼å¼
    for i, pred in enumerate(predictions):
        validate_function_call_format(pred, "predictions", i)
    
    for i, ref in enumerate(references):
        validate_function_call_format(ref, "references", i)
    
    # 8. åˆ›å»ºå‚è€ƒç»“æœçš„å‡½æ•°åç§°é›†åˆï¼Œä¾¿äºå¿«é€ŸæŸ¥æ‰¾
    reference_function_names = {ref["function"]["name"] for ref in references}
    
    # 9. æ‰¾å‡ºäº¤é›†ï¼špredictions ä¸­çš„å‡½æ•°åç§°åœ¨ references ä¸­å­˜åœ¨çš„è®°å½•
    matched_calls = []
    for pred in predictions:
        function_name = pred["function"]["name"]
        if function_name in reference_function_names:
            matched_calls.append(pred)
    
    # 10. è®¡ç®—å„ç§æŒ‡æ ‡
    intersection_count = len(matched_calls)  # äº¤é›†æ•°é‡
    predictions_count = len(predictions)     # é¢„æµ‹æ•°é‡
    references_count = len(references)       # å‚è€ƒæ•°é‡
    
    # è®¡ç®—å·¥ä½œæµé€‰æ‹©æ­£ç¡®æ€§
    correct_workflow_selection = intersection_count >= number
    
    # è®¡ç®—å¹»è§‰ç‡ï¼š(predictionsç‹¬æœ‰æ•°é‡) / referencesæ•°é‡
    # predictionsç‹¬æœ‰çš„æ•°é‡
    predictions_only_count = predictions_count - intersection_count
    if references_count == 0:
        if predictions_only_count > 0:
            hallucination_rate = float('inf')
        else:
            hallucination_rate = 0.0
    else:
        hallucination_rate = predictions_only_count / references_count
    
    return {
        "fcm": matched_calls,
        "tool_recognition_rate": tool_recognition_rate,
        "correct_workflow_selection": correct_workflow_selection,
        "hallucination_rate": hallucination_rate
    }


def run_tests():
    """è¿è¡Œå•å…ƒæµ‹è¯•å’ŒåŠŸèƒ½æµ‹è¯•"""
    import traceback
    
    print("=" * 60)
    print("å¼€å§‹è¿è¡Œ evaluate_function_calls_metrics å‡½æ•°æµ‹è¯•")
    print("=" * 60)
    
    passed_count = 0
    
    # æµ‹è¯•ç”¨ä¾‹1ï¼šæ­£å¸¸æƒ…å†µ - æœ‰äº¤é›†
    def test_normal_intersection():
        predictions = [
            {
                "id": "chatcmpl-tool-1f7c0ca26c8240d98a54db95c9a3b369",
                "type": "function",
                "function": {
                    "name": "ä¼ä¸šé‡‘èè¡Œä¸ºæ™ºèƒ½ä½“-V3",
                    "arguments": "{\"reason\": \"ç”¨æˆ·è¯¢é—®åŒ—äº¬æºç èµ„æœ¬æŠ•èµ„æœ‰é™å…¬å¸è‡ª2023å¹´ä»¥æ¥é€€å‡ºçš„é£é™©æŠ•èµ„äº‹ä»¶\"}"
                }
            },
            {
                "id": "chatcmpl-tool-51300ca26c8240018a54db95c9a3cdef",
                "type": "function", 
                "function": {
                    "name": "æ™ºèƒ½ä½“-1",
                    "arguments": "{\"reason\": \"ç”¨æˆ·è¯¢é—®xxå…¬å¸è‡ª2020å¹´ä»¥æ¥é€€å‡ºçš„é£é™©æŠ•èµ„äº‹ä»¶\"}"
                }
            }
        ]
        
        references = [
            {
                "id": "chatcmpl-tool-1f7c0ca26c8240d98a54db95c9a3b369",
                "type": "function",
                "function": {
                    "name": "ä¼ä¸šé‡‘èè¡Œä¸ºæ™ºèƒ½ä½“-V3",
                    "arguments": "{\"reason\": \"ç”¨æˆ·è¯¢é—®åŒ—äº¬æºç èµ„æœ¬æŠ•èµ„æœ‰é™å…¬å¸è‡ª2023å¹´ä»¥æ¥é€€å‡ºçš„é£é™©æŠ•èµ„äº‹ä»¶\"}"
                }
            },
            {
                "id": "chatcmpl-tool-2a3b0ca26c8240018a54db95c9a3a9b6",
                "type": "function",
                "function": {
                    "name": "æ™ºèƒ½ä½“-5", 
                    "arguments": "{\"reason\": \"ç”¨æˆ·è¯¢é—®xxå…¬å¸è‡ª2020å¹´ä»¥æ¥é€€å‡ºçš„é£é™©æŠ•èµ„äº‹ä»¶\"}"
                }
            }
        ]
        
        result = evaluate_function_calls_metrics(predictions, references)
        
        # éªŒè¯ç»“æœ
        assert "fcm" in result, "ç»“æœå¿…é¡»åŒ…å« fcm å­—æ®µ"
        assert len(result["fcm"]) == 1, f"é¢„æœŸäº¤é›†é•¿åº¦ä¸º1ï¼Œå®é™…ä¸º{len(result['fcm'])}"
        assert result["fcm"][0]["function"]["name"] == "ä¼ä¸šé‡‘èè¡Œä¸ºæ™ºèƒ½ä½“-V3", "äº¤é›†å…ƒç´ å‡½æ•°åä¸åŒ¹é…"
        
        # éªŒè¯æ–°å¢æŒ‡æ ‡
        assert result["tool_recognition_rate"] == True, "å·¥å…·è¯†åˆ«ç‡åº”ä¸ºTrue"
        # äº¤é›†æ•°é‡ä¸º1ï¼Œé»˜è®¤number=1ï¼Œ1 >= 1 ä¸ºTrue
        assert result["correct_workflow_selection"] == True, "å·¥ä½œæµé€‰æ‹©åº”ä¸ºæ­£ç¡®ï¼ˆäº¤é›†æ•°é‡1ç­‰äºé˜ˆå€¼1ï¼‰"
        # å¹»è§‰ç‡ï¼špredictions=2ï¼Œäº¤é›†=1ï¼Œreferences=2ï¼Œ(2-1)/2=0.5
        assert result["hallucination_rate"] == 0.5, f"å¹»è§‰ç‡åº”ä¸º0.5ï¼Œå®é™…ä¸º{result['hallucination_rate']}"
        
        return True, "æ­£å¸¸æƒ…å†µæµ‹è¯•é€šè¿‡"
    
    # æµ‹è¯•ç”¨ä¾‹2ï¼šç©ºåˆ—è¡¨æƒ…å†µ
    def test_empty_lists():
        # predictions ä¸ºç©ºï¼Œreferences ä¸ä¸ºç©º
        test_ref = [{"id": "1", "type": "function", 
                     "function": {"name": "test", "arguments": "{}"}}]
        result1 = evaluate_function_calls_metrics([], test_ref)
        expected1 = {
            "fcm": [],
            "tool_recognition_rate": False,
            "correct_workflow_selection": False,
            "hallucination_rate": 0.0  # predictionsä¸ºç©ºï¼Œæ²¡æœ‰é¢„æµ‹å°±æ²¡æœ‰å¹»è§‰
        }
        assert result1 == expected1, (
            f"predictions ä¸ºç©ºæ—¶ç»“æœä¸æ­£ç¡®ï¼ŒæœŸæœ›: {expected1}ï¼Œå®é™…: {result1}"
        )
        
        # predictions ä¸ä¸ºç©ºï¼Œreferences ä¸ºç©º  
        test_pred = [{"id": "1", "type": "function", 
                      "function": {"name": "test", "arguments": "{}"}}]
        result2 = evaluate_function_calls_metrics(test_pred, [])
        expected2 = {
            "fcm": [],
            "tool_recognition_rate": True,
            "correct_workflow_selection": False,
            "hallucination_rate": float('inf')  # referencesä¸ºç©ºï¼Œæ‰€æœ‰predictionséƒ½æ˜¯å¹»è§‰
        }
        assert result2 == expected2, (
            f"references ä¸ºç©ºæ—¶ç»“æœä¸æ­£ç¡®ï¼ŒæœŸæœ›: {expected2}ï¼Œå®é™…: {result2}"
        )
        
        # éƒ½ä¸ºç©º
        result3 = evaluate_function_calls_metrics([], [])
        expected3 = {
            "fcm": [],
            "tool_recognition_rate": False,
            "correct_workflow_selection": False,
            "hallucination_rate": 0.0  # predictionsä¸ºç©ºï¼Œæ²¡æœ‰é¢„æµ‹å°±æ²¡æœ‰å¹»è§‰
        }
        assert result3 == expected3, (
            f"éƒ½ä¸ºç©ºæ—¶ç»“æœä¸æ­£ç¡®ï¼ŒæœŸæœ›: {expected3}ï¼Œå®é™…: {result3}"
        )
        
        return True, "ç©ºåˆ—è¡¨æƒ…å†µæµ‹è¯•é€šè¿‡"
    
    # æµ‹è¯•ç”¨ä¾‹3ï¼šæ— äº¤é›†æƒ…å†µ
    def test_no_intersection():
        predictions = [{"id": "1", "type": "function", 
                        "function": {"name": "æ™ºèƒ½ä½“-A", "arguments": "{}"}}]
        references = [{"id": "2", "type": "function", 
                       "function": {"name": "æ™ºèƒ½ä½“-B", "arguments": "{}"}}]
        
        result = evaluate_function_calls_metrics(predictions, references)
        expected = {
            "fcm": [],
            "tool_recognition_rate": True,
            "correct_workflow_selection": False,
            "hallucination_rate": 1.0  # 1ä¸ªpredictionsï¼Œ0ä¸ªäº¤é›†ï¼Œ1ä¸ªreferencesï¼š1/1=1.0
        }
        assert result == expected, (
            f"æ— äº¤é›†æ—¶ç»“æœä¸æ­£ç¡®ï¼ŒæœŸæœ›: {expected}ï¼Œå®é™…: {result}"
        )
        
        return True, "æ— äº¤é›†æƒ…å†µæµ‹è¯•é€šè¿‡"
    
    # æµ‹è¯•ç”¨ä¾‹4ï¼šå‚æ•°ç±»å‹é”™è¯¯
    def test_type_errors():
        try:
            evaluate_function_calls_metrics("not_a_list", [])
            assert False, "åº”è¯¥æŠ›å‡º TypeError"
        except TypeError as e:
            assert "predictions å¿…é¡»æ˜¯ list ç±»å‹" in str(e)
        
        try:
            evaluate_function_calls_metrics([], "not_a_list")
            assert False, "åº”è¯¥æŠ›å‡º TypeError"
        except TypeError as e:
            assert "references å¿…é¡»æ˜¯ list ç±»å‹" in str(e)
        
        try:
            evaluate_function_calls_metrics([], [], "not_an_int")
            assert False, "åº”è¯¥æŠ›å‡º TypeError"
        except TypeError as e:
            assert "number å¿…é¡»æ˜¯ int ç±»å‹" in str(e)
        
        # æµ‹è¯•è´Ÿæ•°numberå‚æ•°
        try:
            evaluate_function_calls_metrics([], [], -5)
            assert False, "åº”è¯¥æŠ›å‡º ValueError"
        except ValueError as e:
            assert "number å¿…é¡»æ˜¯éè´Ÿæ•°" in str(e)
        
        return True, "ç±»å‹é”™è¯¯æµ‹è¯•é€šè¿‡"
    
    # æµ‹è¯•ç”¨ä¾‹5ï¼šæ ¼å¼æ ¡éªŒé”™è¯¯
    def test_format_validation():
        # ç¼ºå°‘å¿…éœ€å­—æ®µ
        try:
            invalid_pred = [{"id": "1"}]
            valid_ref = [{"id": "2", "type": "function", 
                          "function": {"name": "test", "arguments": "{}"}}]
            evaluate_function_calls_metrics(invalid_pred, valid_ref)
            assert False, "åº”è¯¥æŠ›å‡º ValueError"
        except ValueError as e:
            assert "ç¼ºå°‘å¿…éœ€å­—æ®µ" in str(e)
        
        # type å­—æ®µå€¼é”™è¯¯
        try:
            invalid_type_pred = [{"id": "1", "type": "invalid", 
                                  "function": {"name": "test", "arguments": "{}"}}]
            valid_ref = [{"id": "2", "type": "function", 
                          "function": {"name": "test", "arguments": "{}"}}]
            evaluate_function_calls_metrics(invalid_type_pred, valid_ref)
            assert False, "åº”è¯¥æŠ›å‡º ValueError"
        except ValueError as e:
            assert "type å­—æ®µå¿…é¡»ä¸º 'function'" in str(e)
        
        # arguments ä¸æ˜¯æœ‰æ•ˆ JSON
        try:
            invalid_json_pred = [{"id": "1", "type": "function", 
                                  "function": {"name": "test", 
                                              "arguments": "invalid_json"}}]
            valid_ref = [{"id": "2", "type": "function", 
                          "function": {"name": "test", "arguments": "{}"}}]
            evaluate_function_calls_metrics(invalid_json_pred, valid_ref)
            assert False, "åº”è¯¥æŠ›å‡º ValueError"  
        except ValueError as e:
            assert "ä¸æ˜¯æœ‰æ•ˆçš„ JSON å­—ç¬¦ä¸²" in str(e)
        
        return True, "æ ¼å¼æ ¡éªŒæµ‹è¯•é€šè¿‡"
    
    # æµ‹è¯•ç”¨ä¾‹6ï¼šå®Œå…¨åŒ¹é…æƒ…å†µ
    def test_full_match():
        data = [
            {"id": "1", "type": "function", 
             "function": {"name": "æ™ºèƒ½ä½“-A", "arguments": "{}"}},
            {"id": "2", "type": "function", 
             "function": {"name": "æ™ºèƒ½ä½“-B", "arguments": "{}"}}
        ]
        
        result = evaluate_function_calls_metrics(data, data)
        assert len(result["fcm"]) == 2, "å®Œå…¨åŒ¹é…æ—¶åº”è¿”å›æ‰€æœ‰å…ƒç´ "
        assert result["tool_recognition_rate"] is True, "å·¥å…·è¯†åˆ«ç‡åº”ä¸ºTrue"
        assert result["correct_workflow_selection"] is True, "å·¥ä½œæµé€‰æ‹©åº”ä¸ºæ­£ç¡®"
        assert result["hallucination_rate"] == 0.0, "å¹»è§‰ç‡åº”ä¸º0.0"
        
        return True, "å®Œå…¨åŒ¹é…æµ‹è¯•é€šè¿‡"
    
    # æµ‹è¯•ç”¨ä¾‹7ï¼šæ–°æŒ‡æ ‡ä¸“é¡¹æµ‹è¯•
    def test_new_metrics():
        # æµ‹è¯• number å‚æ•°çš„å½±å“
        predictions = [
            {"id": "1", "type": "function", 
             "function": {"name": "æ™ºèƒ½ä½“-A", "arguments": "{}"}},
            {"id": "2", "type": "function", 
             "function": {"name": "æ™ºèƒ½ä½“-B", "arguments": "{}"}}
        ]
        references = [
            {"id": "3", "type": "function", 
             "function": {"name": "æ™ºèƒ½ä½“-A", "arguments": "{}"}},
            {"id": "4", "type": "function", 
             "function": {"name": "æ™ºèƒ½ä½“-C", "arguments": "{}"}}
        ]
        
        # number=1 æ—¶ï¼Œäº¤é›†æ•°é‡1 >= 1 ä¸ºTrue
        result1 = evaluate_function_calls_metrics(
            predictions, references, number=1
        )
        assert result1["correct_workflow_selection"] is True, (
            "number=1æ—¶å·¥ä½œæµé€‰æ‹©åº”ä¸ºTrue"
        )
        
        # number=0 æ—¶ï¼Œäº¤é›†æ•°é‡1 >= 0 ä¸ºTrue
        result2 = evaluate_function_calls_metrics(
            predictions, references, number=0
        )
        assert result2["correct_workflow_selection"] is True, (
            "number=0æ—¶å·¥ä½œæµé€‰æ‹©åº”ä¸ºTrue"
        )
        
        # number=2 æ—¶ï¼Œäº¤é›†æ•°é‡1 >= 2 ä¸ºFalse
        result3 = evaluate_function_calls_metrics(
            predictions, references, number=2
        )
        assert result3["correct_workflow_selection"] is False, (
            "number=2æ—¶å·¥ä½œæµé€‰æ‹©åº”ä¸ºFalse"
        )
        
        # æµ‹è¯•å¹»è§‰ç‡è®¡ç®—ï¼špredictions=2ï¼Œäº¤é›†=1ï¼Œreferences=2ï¼Œå¹»è§‰ç‡=(2-1)/2=0.5
        assert result1["hallucination_rate"] == 0.5, (
            f"å¹»è§‰ç‡åº”ä¸º0.5ï¼Œå®é™…ä¸º{result1['hallucination_rate']}"
        )
        
        return True, "æ–°æŒ‡æ ‡ä¸“é¡¹æµ‹è¯•é€šè¿‡"
    
    # æµ‹è¯•ç”¨ä¾‹8ï¼šè¾¹ç•Œæƒ…å†µæµ‹è¯•
    def test_edge_cases():
        # æµ‹è¯•referencesä¸º0çš„å¹»è§‰ç‡è®¡ç®—
        predictions = [{"id": "1", "type": "function", 
                        "function": {"name": "æ™ºèƒ½ä½“-A", "arguments": "{}"}}]
        references = []
        
        result = evaluate_function_calls_metrics(predictions, references)
        assert result["hallucination_rate"] == float('inf'), (
            "referencesä¸º0ä¸”æœ‰predictionsæ—¶å¹»è§‰ç‡åº”ä¸ºinf"
        )
        assert result["tool_recognition_rate"] is True, (
            "æœ‰predictionsæ—¶å·¥å…·è¯†åˆ«ç‡åº”ä¸ºTrue"
        )
        assert result["correct_workflow_selection"] is False, (
            "ç©ºreferencesæ—¶å·¥ä½œæµé€‰æ‹©åº”ä¸ºFalse"
        )
        
        # æµ‹è¯•å¤šä¸ªç›¸åŒåç§°å‡½æ•°çš„æƒ…å†µ
        predictions_multi = [
            {"id": "1", "type": "function", 
             "function": {"name": "æ™ºèƒ½ä½“-A", "arguments": "{}"}},
            {"id": "2", "type": "function",  # é‡å¤åç§°
             "function": {"name": "æ™ºèƒ½ä½“-A", "arguments": "{}"}},
            {"id": "3", "type": "function", 
             "function": {"name": "æ™ºèƒ½ä½“-B", "arguments": "{}"}}
        ]
        references_multi = [
            {"id": "4", "type": "function", 
             "function": {"name": "æ™ºèƒ½ä½“-A", "arguments": "{}"}},
            {"id": "5", "type": "function", 
             "function": {"name": "æ™ºèƒ½ä½“-C", "arguments": "{}"}}
        ]
        
        result_multi = evaluate_function_calls_metrics(
            predictions_multi, references_multi
        )
        # åº”è¯¥æœ‰2ä¸ªæ™ºèƒ½ä½“-AåŒ¹é…ï¼ˆéƒ½åŒ¹é…åˆ°referencesä¸­çš„æ™ºèƒ½ä½“-Aï¼‰
        assert len(result_multi["fcm"]) == 2, "é‡å¤åç§°åº”è¯¥éƒ½è¢«åŒ¹é…"
        assert result_multi["fcm"][0]["function"]["name"] == "æ™ºèƒ½ä½“-A"
        assert result_multi["fcm"][1]["function"]["name"] == "æ™ºèƒ½ä½“-A"
        
        # å¹»è§‰ç‡è®¡ç®—ï¼špredictions=3ï¼Œäº¤é›†=2ï¼Œreferences=2ï¼Œå¹»è§‰ç‡=(3-2)/2=0.5
        assert result_multi["hallucination_rate"] == 0.5, (
            f"å¤šé‡åŒ¹é…æ—¶å¹»è§‰ç‡åº”ä¸º0.5ï¼Œå®é™…ä¸º{result_multi['hallucination_rate']}"
        )
        
        return True, "è¾¹ç•Œæƒ…å†µæµ‹è¯•é€šè¿‡"
    
    # æµ‹è¯•ç”¨ä¾‹9ï¼šè´Ÿæ•°å’Œæå€¼numberå‚æ•°æµ‹è¯•
    def test_extreme_number_values():
        predictions = [
            {"id": "1", "type": "function", 
             "function": {"name": "æ™ºèƒ½ä½“-A", "arguments": "{}"}},
            {"id": "2", "type": "function", 
             "function": {"name": "æ™ºèƒ½ä½“-B", "arguments": "{}"}}
        ]
        references = [
            {"id": "3", "type": "function", 
             "function": {"name": "æ™ºèƒ½ä½“-A", "arguments": "{}"}}
        ]
        
        # æµ‹è¯•è´Ÿæ•°numberå‚æ•°åº”è¯¥æŠ›å‡ºå¼‚å¸¸
        try:
            evaluate_function_calls_metrics(
                predictions, references, number=-1
            )
            assert False, "è´Ÿæ•°numberå‚æ•°åº”è¯¥æŠ›å‡ºå¼‚å¸¸"
        except ValueError as e:
            assert "number å¿…é¡»æ˜¯éè´Ÿæ•°" in str(e), f"å¼‚å¸¸ä¿¡æ¯ä¸æ­£ç¡®: {e}"
        
        # æµ‹è¯•æå¤§numberå€¼
        result_large = evaluate_function_calls_metrics(
            predictions, references, number=1000
        )
        assert result_large["correct_workflow_selection"] is False, (
            "æå¤§é˜ˆå€¼æ—¶åº”è¯¥ä¸ºFalseï¼ˆ1 >= 1000ä¸ºFalseï¼‰"
        )
        
        # æµ‹è¯•numberä¸º0çš„è¾¹ç•Œæƒ…å†µ
        result_zero = evaluate_function_calls_metrics(
            predictions, references, number=0
        )
        assert result_zero["correct_workflow_selection"] is True, (
            "é˜ˆå€¼ä¸º0æ—¶åº”è¯¥ä¸ºTrueï¼ˆ1 >= 0ï¼‰"
        )
        
        return True, "æå€¼numberå‚æ•°æµ‹è¯•é€šè¿‡"
    
    # æµ‹è¯•ç”¨ä¾‹10ï¼šå‡½æ•°åè¾¹ç•Œæƒ…å†µæµ‹è¯•
    def test_function_name_boundaries():
        # æµ‹è¯•ç©ºå­—ç¬¦ä¸²å‡½æ•°å
        predictions_empty_name = [
            {"id": "1", "type": "function", 
             "function": {"name": "", "arguments": "{}"}}
        ]
        references_empty_name = [
            {"id": "2", "type": "function", 
             "function": {"name": "", "arguments": "{}"}}
        ]
        
        result_empty = evaluate_function_calls_metrics(
            predictions_empty_name, references_empty_name
        )
        assert len(result_empty["fcm"]) == 1, "ç©ºå­—ç¬¦ä¸²å‡½æ•°ååº”è¯¥èƒ½å¤ŸåŒ¹é…"
        assert result_empty["hallucination_rate"] == 0.0, "å®Œå…¨åŒ¹é…æ—¶å¹»è§‰ç‡åº”ä¸º0"
        
        # æµ‹è¯•åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„å‡½æ•°å
        special_chars_name = "å‡½æ•°@#$%^&*()_+-=[]{}|;':\",./<>?"
        predictions_special = [
            {"id": "1", "type": "function", 
             "function": {"name": special_chars_name, "arguments": "{}"}}
        ]
        references_special = [
            {"id": "2", "type": "function", 
             "function": {"name": special_chars_name, "arguments": "{}"}}
        ]
        
        result_special = evaluate_function_calls_metrics(
            predictions_special, references_special
        )
        assert len(result_special["fcm"]) == 1, "ç‰¹æ®Šå­—ç¬¦å‡½æ•°ååº”è¯¥èƒ½å¤ŸåŒ¹é…"
        
        # æµ‹è¯•Unicodeå­—ç¬¦ï¼ˆemojiç­‰ï¼‰
        unicode_name = "æ™ºèƒ½ä½“ğŸ¤–AIåŠ©æ‰‹âœ¨"
        predictions_unicode = [
            {"id": "1", "type": "function", 
             "function": {"name": unicode_name, "arguments": "{}"}}
        ]
        references_unicode = [
            {"id": "2", "type": "function", 
             "function": {"name": unicode_name, "arguments": "{}"}}
        ]
        
        result_unicode = evaluate_function_calls_metrics(
            predictions_unicode, references_unicode
        )
        assert len(result_unicode["fcm"]) == 1, "Unicodeå­—ç¬¦å‡½æ•°ååº”è¯¥èƒ½å¤ŸåŒ¹é…"
        
        return True, "å‡½æ•°åè¾¹ç•Œæƒ…å†µæµ‹è¯•é€šè¿‡"
    
    # æµ‹è¯•ç”¨ä¾‹11ï¼šå¤§å°å†™æ•æ„Ÿæ€§æµ‹è¯•
    def test_case_sensitivity():
        predictions_lower = [
            {"id": "1", "type": "function", 
             "function": {"name": "aiagent", "arguments": "{}"}}
        ]
        references_upper = [
            {"id": "2", "type": "function", 
             "function": {"name": "AIAGENT", "arguments": "{}"}}
        ]
        
        result = evaluate_function_calls_metrics(
            predictions_lower, references_upper
        )
        # å‡½æ•°ååŒ¹é…åº”è¯¥æ˜¯å¤§å°å†™æ•æ„Ÿçš„
        assert len(result["fcm"]) == 0, "å¤§å°å†™ä¸åŒçš„å‡½æ•°åä¸åº”è¯¥åŒ¹é…"
        assert result["hallucination_rate"] == 1.0, (
            "æ— åŒ¹é…æ—¶å¹»è§‰ç‡åº”ä¸º 1.0 (1/1)"
        )
        
        # æµ‹è¯•å®Œå…¨ç›¸åŒçš„å¤§å°å†™
        predictions_same = [
            {"id": "1", "type": "function", 
             "function": {"name": "AIAgent", "arguments": "{}"}}
        ]
        references_same = [
            {"id": "2", "type": "function", 
             "function": {"name": "AIAgent", "arguments": "{}"}}
        ]
        
        result_same = evaluate_function_calls_metrics(
            predictions_same, references_same
        )
        assert len(result_same["fcm"]) == 1, "ç›¸åŒå¤§å°å†™çš„å‡½æ•°ååº”è¯¥åŒ¹é…"
        
        return True, "å¤§å°å†™æ•æ„Ÿæ€§æµ‹è¯•é€šè¿‡"
    
    # æµ‹è¯•ç”¨ä¾‹12ï¼šå¤æ‚JSON argumentsæµ‹è¯•
    def test_complex_json_arguments():
        # æµ‹è¯•å¤æ‚åµŒå¥—JSON
        complex_json = json.dumps({
            "reason": "æµ‹è¯•å¤æ‚å‚æ•°",
            "parameters": {
                "nested": {
                    "array": [1, 2, 3, {"key": "value"}],
                    "unicode": "æµ‹è¯•ä¸­æ–‡ğŸ‰",
                    "special_chars": "@#$%^&*()"
                },
                "numbers": [1.23, -456, 0],
                "boolean": True,
                "null_value": None
            }
        }, ensure_ascii=False)
        
        predictions_complex = [
            {"id": "1", "type": "function", 
             "function": {"name": "complex-agent", "arguments": complex_json}}
        ]
        references_complex = [
            {"id": "2", "type": "function", 
             "function": {"name": "complex-agent", "arguments": "{}"}}
        ]
        
        # åº”è¯¥èƒ½æ­£å¸¸å¤„ç†å¤æ‚JSONï¼ˆåªçœ‹å‡½æ•°ååŒ¹é…ï¼‰
        result = evaluate_function_calls_metrics(
            predictions_complex, references_complex
        )
        assert len(result["fcm"]) == 1, "å¤æ‚JSONå‚æ•°åº”è¯¥èƒ½æ­£å¸¸å¤„ç†"
        
        # æµ‹è¯•ç©ºJSONå¯¹è±¡
        predictions_empty_json = [
            {"id": "1", "type": "function", 
             "function": {"name": "test-agent", "arguments": "{}"}}
        ]
        references_empty_json = [
            {"id": "2", "type": "function", 
             "function": {"name": "test-agent", "arguments": "{}"}}
        ]
        
        result_empty_json = evaluate_function_calls_metrics(
            predictions_empty_json, references_empty_json
        )
        assert len(result_empty_json["fcm"]) == 1, "ç©ºJSONå¯¹è±¡åº”è¯¥èƒ½æ­£å¸¸å¤„ç†"
        
        return True, "å¤æ‚JSON argumentsæµ‹è¯•é€šè¿‡"
    
    # æµ‹è¯•ç”¨ä¾‹13ï¼šæ•°æ®ç±»å‹è¾¹ç•Œæµ‹è¯•
    def test_data_type_boundaries():
        # æµ‹è¯•function.nameä¸æ˜¯å­—ç¬¦ä¸²çš„æƒ…å†µï¼ˆåº”è¯¥æŠ›å¼‚å¸¸ï¼‰
        try:
            invalid_name_type = [
                {"id": "1", "type": "function", 
                 "function": {"name": 123, "arguments": "{}"}}
            ]
            references = [
                {"id": "2", "type": "function", 
                 "function": {"name": "test", "arguments": "{}"}}
            ]
            # è¿™é‡Œä¸ä¼šæŠ›å¼‚å¸¸ï¼Œå› ä¸ºéªŒè¯é€»è¾‘åªæ£€æŸ¥å­—æ®µå­˜åœ¨ï¼Œä¸æ£€æŸ¥ç±»å‹
            result = evaluate_function_calls_metrics(invalid_name_type, references)
            # ä½†æ•°å­—å’Œå­—ç¬¦ä¸²ä¸ä¼šåŒ¹é…
            assert len(result["fcm"]) == 0, "æ•°å­—ç±»å‹å‡½æ•°åä¸åº”è¯¥åŒ¹é…å­—ç¬¦ä¸²"
        except Exception as e:
            # å¦‚æœæŠ›å¼‚å¸¸ä¹Ÿæ˜¯åˆç†çš„
            pass
        
        # æµ‹è¯•function.argumentsä¸æ˜¯å­—ç¬¦ä¸²ä½†èƒ½è¢«json.loadså¤„ç†çš„æƒ…å†µ
        try:
            invalid_args_type = [
                {"id": "1", "type": "function", 
                 "function": {"name": "test", "arguments": 123}}  # æ•°å­—
            ]
            references = [
                {"id": "2", "type": "function", 
                 "function": {"name": "test", "arguments": "{}"}}
            ]
            evaluate_function_calls_metrics(invalid_args_type, references)
            assert False, "éå­—ç¬¦ä¸²argumentsåº”è¯¥æŠ›å‡ºå¼‚å¸¸"
        except (ValueError, TypeError):
            pass  # é¢„æœŸçš„å¼‚å¸¸
        
        return True, "æ•°æ®ç±»å‹è¾¹ç•Œæµ‹è¯•é€šè¿‡"
    
    # æµ‹è¯•ç”¨ä¾‹14ï¼šå¤§æ•°æ®é‡æ€§èƒ½æµ‹è¯•
    def test_large_dataset_performance():
        import time
        
        # åˆ›å»ºå¤§é‡æ•°æ®è¿›è¡Œæ€§èƒ½æµ‹è¯•
        large_predictions = []
        large_references = []
        
        # åˆ›å»º1000ä¸ªé¢„æµ‹å’Œ800ä¸ªå‚è€ƒï¼Œå…¶ä¸­500ä¸ªåŒ¹é…
        for i in range(1000):
            large_predictions.append({
                "id": f"pred-{i}",
                "type": "function",
                "function": {
                    "name": f"agent-{i % 500}" if i < 500 else f"unique-pred-{i}",
                    "arguments": f"{{\"index\": {i}}}"
                }
            })
        
        for i in range(800):
            large_references.append({
                "id": f"ref-{i}",
                "type": "function",
                "function": {
                    "name": f"agent-{i}" if i < 500 else f"unique-ref-{i}",
                    "arguments": f"{{\"index\": {i}}}"
                }
            })
        
        start_time = time.time()
        result = evaluate_function_calls_metrics(large_predictions, large_references)
        end_time = time.time()
        
        # éªŒè¯ç»“æœæ­£ç¡®æ€§
        assert len(result["fcm"]) == 500, f"åº”è¯¥æœ‰500ä¸ªåŒ¹é…ï¼Œå®é™…æœ‰{len(result['fcm'])}"
        assert result["tool_recognition_rate"] is True, "å¤§é‡æ•°æ®æ—¶å·¥å…·è¯†åˆ«ç‡åº”ä¸ºTrue"
        
        # éªŒè¯æ€§èƒ½ï¼ˆåº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆï¼Œæ¯”å¦‚1ç§’ï¼‰
        execution_time = end_time - start_time
        assert execution_time < 2.0, f"å¤§æ•°æ®é‡å¤„ç†æ—¶é—´è¿‡é•¿: {execution_time:.3f}ç§’"
        
        # éªŒè¯å¹»è§‰ç‡è®¡ç®—ï¼špredictions=1000ï¼Œmatched=500ï¼Œreferences=800
        # hallucination_rate = (1000-500)/800 = 0.625
        expected_hallucination_rate = 0.625
        assert abs(result["hallucination_rate"] - expected_hallucination_rate) < 1e-10, (
            f"å¤§æ•°æ®é‡å¹»è§‰ç‡è®¡ç®—é”™è¯¯ï¼ŒæœŸæœ›{expected_hallucination_rate}ï¼Œ"
            f"å®é™…{result['hallucination_rate']}"
        )
        
        return True, "å¤§æ•°æ®é‡æ€§èƒ½æµ‹è¯•é€šè¿‡"
    
    # æµ‹è¯•ç”¨ä¾‹15ï¼šå¼‚å¸¸æ¢å¤å’Œé²æ£’æ€§æµ‹è¯•
    def test_robustness():
        # æµ‹è¯•åŒ…å«Noneå€¼çš„æƒ…å†µ
        try:
            predictions_with_none = [
                {"id": "1", "type": "function", 
                 "function": {"name": "test", "arguments": None}}
            ]
            references = [
                {"id": "2", "type": "function", 
                 "function": {"name": "test", "arguments": "{}"}}
            ]
            evaluate_function_calls_metrics(predictions_with_none, references)
            assert False, "Noneç±»å‹çš„argumentsåº”è¯¥æŠ›å‡ºå¼‚å¸¸"
        except (ValueError, TypeError):
            pass  # é¢„æœŸçš„å¼‚å¸¸
        
        # æµ‹è¯•æ··åˆæœ‰æ•ˆå’Œæ— æ•ˆæ•°æ®ï¼ˆéƒ¨åˆ†æ•°æ®æœ‰æ•ˆçš„æƒ…å†µï¼‰
        # è¿™ç§æƒ…å†µä¸‹ï¼Œæ— æ•ˆæ•°æ®åº”è¯¥åœ¨éªŒè¯é˜¶æ®µè¢«æ•è·ï¼Œä¸åº”è¯¥è¿›è¡Œéƒ¨åˆ†å¤„ç†
        try:
            mixed_predictions = [
                {"id": "1", "type": "function", 
                 "function": {"name": "valid-agent", "arguments": "{}"}},
                {"id": "2", "type": "function", 
                 "function": {"name": "invalid-agent"}}  # ç¼ºå°‘argumentså­—æ®µ
            ]
            references = [
                {"id": "3", "type": "function", 
                 "function": {"name": "valid-agent", "arguments": "{}"}}
            ]
            evaluate_function_calls_metrics(mixed_predictions, references)
            assert False, "æ··åˆæœ‰æ•ˆæ— æ•ˆæ•°æ®åº”è¯¥æŠ›å‡ºå¼‚å¸¸"
        except ValueError:
            pass  # é¢„æœŸçš„å¼‚å¸¸
        
        return True, "å¼‚å¸¸æ¢å¤å’Œé²æ£’æ€§æµ‹è¯•é€šè¿‡"
    
    # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    test_functions = [
        ("æ­£å¸¸æƒ…å†µæµ‹è¯•", test_normal_intersection),
        ("ç©ºåˆ—è¡¨æµ‹è¯•", test_empty_lists), 
        ("æ— äº¤é›†æµ‹è¯•", test_no_intersection),
        ("ç±»å‹é”™è¯¯æµ‹è¯•", test_type_errors),
        ("æ ¼å¼æ ¡éªŒæµ‹è¯•", test_format_validation),
        ("å®Œå…¨åŒ¹é…æµ‹è¯•", test_full_match),
        ("æ–°æŒ‡æ ‡ä¸“é¡¹æµ‹è¯•", test_new_metrics),
        ("è¾¹ç•Œæƒ…å†µæµ‹è¯•", test_edge_cases),
        ("æå€¼numberå‚æ•°æµ‹è¯•", test_extreme_number_values),
        ("å‡½æ•°åè¾¹ç•Œæƒ…å†µæµ‹è¯•", test_function_name_boundaries),
        ("å¤§å°å†™æ•æ„Ÿæ€§æµ‹è¯•", test_case_sensitivity),
        ("å¤æ‚JSON argumentsæµ‹è¯•", test_complex_json_arguments),
        ("æ•°æ®ç±»å‹è¾¹ç•Œæµ‹è¯•", test_data_type_boundaries),
        ("å¤§æ•°æ®é‡æ€§èƒ½æµ‹è¯•", test_large_dataset_performance),
        ("å¼‚å¸¸æ¢å¤å’Œé²æ£’æ€§æµ‹è¯•", test_robustness)
    ]
    
    for test_name, test_func in test_functions:
        try:
            success, message = test_func()
            if success:
                print(f"âœ… {test_name}: {message}")
                passed_count += 1
            else:
                print(f"âŒ {test_name}: {message}")
        except Exception as e:
            print(f"âŒ {test_name}: å¼‚å¸¸ - {str(e)}")
            traceback.print_exc()
    
    print(f"\næµ‹è¯•æ€»ç»“: {passed_count}/{len(test_functions)} ä¸ªæµ‹è¯•ç”¨ä¾‹é€šè¿‡")
    
    # åŠŸèƒ½æ¼”ç¤º
    print("\n" + "=" * 60)
    print("åŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    
    demo_predictions = [
        {
            "id": "chatcmpl-tool-demo1",
            "type": "function",
            "function": {
                "name": "ä¼ä¸šé‡‘èè¡Œä¸ºæ™ºèƒ½ä½“-V3",
                "arguments": "{\"reason\": \"æ¼”ç¤ºæŸ¥è¯¢åŠŸèƒ½\"}"
            }
        },
        {
            "id": "chatcmpl-tool-demo2", 
            "type": "function",
            "function": {
                "name": "æ™ºèƒ½ä½“-æ¼”ç¤º",
                "arguments": "{\"query\": \"æ¼”ç¤ºå‚æ•°\"}"
            }
        }
    ]
    
    demo_references = [
        {
            "id": "chatcmpl-tool-ref1",
            "type": "function", 
            "function": {
                "name": "ä¼ä¸šé‡‘èè¡Œä¸ºæ™ºèƒ½ä½“-V3",
                "arguments": "{\"reason\": \"å‚è€ƒæŸ¥è¯¢åŠŸèƒ½\"}"
            }
        },
        {
            "id": "chatcmpl-tool-ref2",
            "type": "function",
            "function": {
                "name": "å…¶ä»–æ™ºèƒ½ä½“",
                "arguments": "{\"param\": \"å…¶ä»–å‚æ•°\"}"
            }
        }
    ]
    
    demo_result = evaluate_function_calls_metrics(demo_predictions, demo_references)
    
    print("æ¼”ç¤ºè¾“å…¥:")
    print(f"predictions: {len(demo_predictions)} ä¸ªå‡½æ•°è°ƒç”¨")
    print(f"references: {len(demo_references)} ä¸ªå‡½æ•°è°ƒç”¨") 
    print(f"\næ¼”ç¤ºè¾“å‡º:")
    print(f"åŒ¹é…ç»“æœ: {len(demo_result['fcm'])} ä¸ªå‡½æ•°è°ƒç”¨åŒ¹é…")
    if demo_result['fcm']:
        for i, match in enumerate(demo_result['fcm']):
            print(f"  åŒ¹é… {i+1}: {match['function']['name']}")
    
    print(f"\næŒ‡æ ‡è¯„ä¼°:")
    print(f"å·¥å…·è¯†åˆ«ç‡: {demo_result['tool_recognition_rate']}")
    print(f"æ­£ç¡®å·¥ä½œæµé€‰æ‹©: {demo_result['correct_workflow_selection']}")
    print(f"å¹»è§‰ç‡: {demo_result['hallucination_rate']}")
    
    # é¢å¤–æ¼”ç¤ºä¸åŒnumberå‚æ•°çš„å½±å“  
    print(f"\nä¸åŒé˜ˆå€¼ä¸‹çš„å·¥ä½œæµé€‰æ‹©ç»“æœï¼ˆäº¤é›†æ•°é‡ä¸º1ï¼‰:")
    for threshold in [0, 1, 2]:
        result_with_threshold = evaluate_function_calls_metrics(
            demo_predictions, demo_references, number=threshold
        )
        print(
            f"  é˜ˆå€¼ {threshold}: "
            f"{result_with_threshold['correct_workflow_selection']} "
            f"(1 >= {threshold})"
        )


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    run_tests()
